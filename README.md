# RAG QA Chatbot  

A clean and practical Retrieval-Augmented Generation (RAG) chatbot built with **LangChain**, **OpenAI**, and **Pinecone**.  
The goal of this project is to provide a simple, production-friendly reference implementation for document ingestion, embeddings, vector search, and LLM-based question answering.  

---  

## ğŸ“ Why This Exists  
Most RAG examples online are either artificial or overly complicated.  
This repo shows a **minimal but realistic** workflow you can extend in real environments.  

---  

## ğŸ§  What It Does  
- Takes your documents  
- Cleans + chunks text  
- Generates embeddings  
- Stores them in Pinecone  
- Retrieves relevant chunks  
- Sends context + query to an LLM  
- Returns an accurate answer  

---  

## ğŸ— Tech Stack  
- **Python**  
- **LangChain**  
- **OpenAI API**  
- **Pinecone**  
- **FastAPI**  
- **Uvicorn**  
- **Sentence Transformers (optional local embeddings)**  

---  

## ğŸ¥¥ Architecture  

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Raw Documents    â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â”‚  Ingestion + Chunking
                          â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Embedding Model  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â”‚  Upsert vectors
                          â”‚
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  Vector Store   â”‚
                  â”‚   (Pinecone)    â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â”‚  Retrieve top-k chunks
                          â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚      RAG Engine       â”‚
              â”‚ (LLM + Context Merge) â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â”‚  API Response
                          â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   FastAPI Server   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---  

## ğŸš€ Getting Started  

### 1. Clone the repo  
```bash
git clone https://github.com/your-username/rag-qa-chatbot
cd rag-qa-chatbot
```

### 2. Install dependencies  
```
pip install -r requirements.txt
```

### 3. Set up environment variables  
Copy the example file:
```
cp .env.example .env
```
Fill in:  
- `OPENAI_API_KEY`  
- `PINECONE_API_KEY`

### 4. Run the API  

```
uvicorn src.app:app --reload
```

Open browser:  
```
http://127.0.0.1:8000/docs
```

---  

## ğŸ¦– Example Query  

```
curl -X POST "http://localhost:8000/ask" \
     -H "Content-Type: application/json" \
     -d '{"question": "What is the refund policy?"}'
```

**Response**

```json
{
  "answer": "Based on the documents, refunds are available within 30 days..."
}
```

---  

## ğŸ’‚ Project Structure  

```
rag-qa-chatbot/
â”‚
â”œâ”€ src/
â”‚   â”œâ”€ app.py
â”‚   â”œâ”€ rag_pipeline.py
â”‚   â”œâ”€ embed.py
â”‚   â”œâ”€ ingest.py
â”‚
â”œâ”€ tests/
â”‚   â””â”€ test_basic.py
â”‚
â”œâ”€ data/
â”‚   â””â”€ README.md
â”‚
â”œâ”€ docs/
â”‚   â””â”€ architecture.md
â”‚
â”œâ”€ scripts/
â”‚   â””â”€ ingest_data.py
â”‚
â”œâ”€ .github/workflows/ci.yml
â”œâ”€ .env.example
â”œâ”€ requirements.txt
â”œâ”€ Dockerfile
â”œâ”€ Makefile
â”œâ”€ LICENSE
â”œâ”€ CHANGELOG.md
â”œâ”€ CODE_OF_CONDUCT.md
â”œâ”€ CONTRIBUTING.md
â””â”€ SECURITY.md
```

---  

## ğŸ“ˆ Future Improvements  
- Add reranking layer  
- Add evaluation dashboard  
- Add hybrid search (keyword + semantic)  
- Add streaming chat UI  

---  

## ğŸ’œ License  
MIT â€” use freely for personal or commercial work.
